"""–î–≤–∏–∂–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π: –ø–æ–∏—Å–∫, –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç–µ–π, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ DeepSeek"""

import asyncio
import logging
from datetime import datetime, timezone
from dateutil import parser as date_parser
from openai import AsyncOpenAI
from duckduckgo_search import DDGS
import aiohttp
from newspaper import Article

from config import (
    DEEPSEEK_API_KEY, DEEPSEEK_BASE_URL, DEEPSEEK_MODEL,
    PRESET_TOPICS, LANGUAGE_LEVELS, WORDS_PER_MINUTE,
    MAX_SEARCH_RESULTS_PER_TOPIC, MAX_ARTICLE_LENGTH, REQUEST_TIMEOUT,
)

logger = logging.getLogger(__name__)

# DeepSeek –∫–ª–∏–µ–Ω—Ç (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π)
client = AsyncOpenAI(
    api_key=DEEPSEEK_API_KEY,
    base_url=DEEPSEEK_BASE_URL,
)


def search_news(query: str, max_results: int = MAX_SEARCH_RESULTS_PER_TOPIC, since: datetime = None) -> list[dict]:
    """–ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ DuckDuckGo —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –¥–∞—Ç–µ"""
    try:
        with DDGS() as ddgs:
            results = list(ddgs.news(query, max_results=max_results * 2, region="wt-wt"))

            if since:
                filtered = []
                for r in results:
                    try:
                        news_date = date_parser.parse(r.get("date", ""))
                        if news_date.tzinfo is None:
                            news_date = news_date.replace(tzinfo=timezone.utc)
                        if since.tzinfo is None:
                            since = since.replace(tzinfo=timezone.utc)
                        if news_date > since:
                            filtered.append(r)
                    except Exception:
                        # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É ‚Äî –≤–∫–ª—é—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å
                        filtered.append(r)
                return filtered[:max_results]

            return results[:max_results]
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ '{query}': {e}")
        return []


async def parse_article(session: aiohttp.ClientSession, url: str) -> dict | None:
    """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)) as resp:
            if resp.status != 200:
                return None
            html = await resp.text()

        article = Article(url)
        article.download(input_html=html)
        article.parse()

        text = article.text.strip()
        if len(text) < 100:
            return None

        return {
            "title": article.title or "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞",
            "text": text[:MAX_ARTICLE_LENGTH],
            "url": url,
            "source": url.split("/")[2] if "/" in url else url,
        }
    except Exception as e:
        logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å {url}: {e}")
        return None


async def fetch_articles_for_topic(
    session: aiohttp.ClientSession,
    topic_name: str,
    search_query: str,
    since: datetime = None,
) -> list[dict]:
    """–°–æ–±—Ä–∞—Ç—å —Å—Ç–∞—Ç—å–∏ –ø–æ –æ–¥–Ω–æ–π —Ç–µ–º–µ"""
    # –ü–æ–∏—Å–∫ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ (duckduckgo_search —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è)
    loop = asyncio.get_event_loop()
    search_results = await loop.run_in_executor(
        None, lambda: search_news(search_query, MAX_SEARCH_RESULTS_PER_TOPIC, since)
    )

    if not search_results:
        return []

    # –ü–∞—Ä—Å–∏–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    urls = [r.get("url") or r.get("href", "") for r in search_results if r.get("url") or r.get("href")]
    tasks = [parse_article(session, url) for url in urls[:MAX_SEARCH_RESULTS_PER_TOPIC]]
    articles = await asyncio.gather(*tasks)

    result = []
    for art in articles:
        if art:
            art["topic"] = topic_name
            result.append(art)

    return result


def build_search_queries(enabled_topics: list, custom_topics: list, lang: str = "ru") -> list[tuple[str, str]]:
    """–°—Ç—Ä–æ–∏–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ —Ç–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    queries = []
    today = datetime.now().strftime("%Y-%m")

    for topic_id in enabled_topics:
        if topic_id in PRESET_TOPICS:
            topic = PRESET_TOPICS[topic_id]
            name = topic["name_ru"] if lang == "ru" else topic["name_en"]
            if lang == "ru":
                q = f"{name} –Ω–æ–≤–æ—Å—Ç–∏ {today}"
            else:
                q = f"{name} news {today}"
            queries.append((name, q))

    for custom in custom_topics:
        if lang == "ru":
            q = f"{custom} –Ω–æ–≤–æ—Å—Ç–∏ {today}"
        else:
            q = f"{custom} news {today}"
        queries.append((custom, q))

    return queries


async def collect_all_news(
    enabled_topics: list,
    custom_topics: list,
    lang: str = "ru",
    since: datetime = None,
) -> list[dict]:
    """–°–æ–±—Ä–∞—Ç—å –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –≤—Å–µ–º —Ç–µ–º–∞–º"""
    queries = build_search_queries(enabled_topics, custom_topics, lang)

    if not queries:
        return []

    connector = aiohttp.TCPConnector(limit=10, ssl=False)
    async with aiohttp.ClientSession(
        connector=connector,
        headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    ) as session:
        tasks = [
            fetch_articles_for_topic(session, topic_name, query, since)
            for topic_name, query in queries
        ]
        results = await asyncio.gather(*tasks)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏
    all_articles = []
    for articles in results:
        all_articles.extend(articles)

    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ URL
    seen_urls = set()
    unique = []
    for art in all_articles:
        if art["url"] not in seen_urls:
            seen_urls.add(art["url"])
            unique.append(art)

    return unique


def build_prompt(
    articles: list[dict],
    language_level: str,
    reading_time: int,
    digest_lang: str,
    important_only: bool = False,
    importance_level: str = "medium",
) -> str:
    """–°–æ–±—Ä–∞—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è DeepSeek"""
    target_words = reading_time * WORDS_PER_MINUTE
    level_prompt = LANGUAGE_LEVELS.get(language_level, LANGUAGE_LEVELS["medium"])["prompt"]

    lang_instruction = "–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ." if digest_lang == "ru" else "Respond in English."

    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç—ã —Å—Ç–∞—Ç–µ–π
    articles_text = ""
    for i, art in enumerate(articles, 1):
        articles_text += f"\n--- –°—Ç–∞—Ç—å—è {i} ---\n"
        articles_text += f"–¢–µ–º–∞: {art['topic']}\n"
        articles_text += f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {art['title']}\n"
        articles_text += f"–ò—Å—Ç–æ—á–Ω–∏–∫: {art['source']}\n"
        articles_text += f"URL: {art['url']}\n"
        articles_text += f"–¢–µ–∫—Å—Ç: {art['text']}\n"

    important_instruction = ""
    if important_only:
        level_map = {
            "low": "–í–∫–ª—é—á–∏ –≤—Å–µ –±–æ–ª–µ–µ-–º–µ–Ω–µ–µ –∑–Ω–∞—á–∏–º—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (10-15 —à—Ç—É–∫).",
            "medium": "–í—ã–±–µ—Ä–∏ —Ç–æ–ª—å–∫–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –≤–∞–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ (5-7 —à—Ç—É–∫).",
            "high": "–¢–æ–ª—å–∫–æ —Å–∞–º—ã–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ, —Ç–æ–ø–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–Ω—è (3-5 —à—Ç—É–∫).",
        }
        important_instruction = f"""
–†–ï–ñ–ò–ú "–¢–û–õ–¨–ö–û –í–ê–ñ–ù–û–ï": {level_map.get(importance_level, level_map["medium"])}
–û—Ç–±–∏—Ä–∞–π –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ä–µ–∞–ª—å–Ω–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –∏ –≤–ª–∏—è–Ω–∏—é –Ω–∞ –º–∏—Ä/–æ—Ç—Ä–∞—Å–ª—å.
"""

    prompt = f"""–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Ä–µ–¥–∞–∫—Ç–æ—Ä. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –¥–∞–π–¥–∂–µ—Å—Ç.

–ü–†–ê–í–ò–õ–ê:
1. –í–∫–ª—é—á–∞–π –¢–û–õ–¨–ö–û –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –≤ –æ–¥–Ω–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–µ –∏ –≤—ã–≥–ª—è–¥–∏—Ç —Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω–æ ‚Äî –æ—Ç–º–µ—Ç—å —ç—Ç–æ.
2. –£–±–µ—Ä–∏ –≤—Å—é "–≤–æ–¥—É": –º–Ω–µ–Ω–∏—è, —Å–ø–µ–∫—É–ª—è—Ü–∏–∏, –∫–ª–∏–∫–±–µ–π—Ç, —Ä–µ–∫–ª–∞–º—É.
3. –ì—Ä—É–ø–ø–∏—Ä—É–π –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ç–µ–º–∞–º.
4. –ö–∞–∂–¥–∞—è –Ω–æ–≤–æ—Å—Ç—å: –∑–∞–≥–æ–ª–æ–≤–æ–∫ + —Å—É—Ç—å –≤ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö + –∏—Å—Ç–æ—á–Ω–∏–∫ (URL).
5. –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–∏—à—É—Ç –æ–± –æ–¥–Ω–æ–º ‚Äî –æ–±—ä–µ–¥–∏–Ω–∏ –∏ —É–∫–∞–∂–∏ –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.
{important_instruction}

–°–¢–ò–õ–¨: {level_prompt}
–Ø–ó–´–ö: {lang_instruction}
–û–ë–™–Å–ú: –ø—Ä–∏–º–µ—Ä–Ω–æ {target_words} —Å–ª–æ–≤ (—á—Ç–µ–Ω–∏–µ ~{reading_time} –º–∏–Ω—É—Ç).

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
–ò—Å–ø–æ–ª—å–∑—É–π —Ç–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç (Telegram MarkdownV2 –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π, –∏—Å–ø–æ–ª—å–∑—É–π HTML):

<b>üìå –ù–ê–ó–í–ê–ù–ò–ï –¢–ï–ú–´</b>

‚ñ∏ <b>–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏</b>
–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å—É—Ç–∏. –ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ, –ø–æ—á–µ–º—É –≤–∞–∂–Ω–æ.
üîó <a href="URL">–ò—Å—Ç–æ—á–Ω–∏–∫</a>

---

–í–û–¢ –°–¢–ê–¢–¨–ò –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{articles_text}

–°–æ–∑–¥–∞–π –¥–∞–π–¥–∂–µ—Å—Ç:"""

    return prompt


async def generate_digest(
    articles: list[dict],
    language_level: str,
    reading_time: int,
    digest_lang: str,
    important_only: bool = False,
    importance_level: str = "medium",
) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–∞ —á–µ—Ä–µ–∑ DeepSeek"""
    if not articles:
        return "üòï –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ç–µ–º–∞–º. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ –∏–ª–∏ –¥–æ–±–∞–≤—å –±–æ–ª—å—à–µ —Ç–µ–º."

    prompt = build_prompt(articles, language_level, reading_time, digest_lang, important_only, importance_level)

    try:
        response = await client.chat.completions.create(
            model=DEEPSEEK_MODEL,
            messages=[
                {"role": "system", "content": "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Ä–µ–¥–∞–∫—Ç–æ—Ä. –¢–≤–æ–∏ –¥–∞–π–¥–∂–µ—Å—Ç—ã —Ç–æ—á–Ω—ã–µ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ –±–µ–∑ –≤–æ–¥—ã."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
            max_tokens=4000,
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ DeepSeek API: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞: {e}"


async def get_news_digest(
    enabled_topics: list,
    custom_topics: list,
    language_level: str = "medium",
    reading_time: int = 7,
    digest_lang: str = "ru",
    important_only: bool = False,
    importance_level: str = "medium",
    last_viewed_at: str = None,
) -> str:
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: –ø–æ–∏—Å–∫ ‚Üí –ø–∞—Ä—Å–∏–Ω–≥ ‚Üí –¥–∞–π–¥–∂–µ—Å—Ç"""
    # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
    since = None
    if last_viewed_at:
        try:
            since = date_parser.parse(last_viewed_at)
        except Exception:
            pass

    articles = await collect_all_news(enabled_topics, custom_topics, digest_lang, since)

    digest = await generate_digest(
        articles=articles,
        language_level=language_level,
        reading_time=reading_time,
        digest_lang=digest_lang,
        important_only=important_only,
        importance_level=importance_level,
    )

    return digest
